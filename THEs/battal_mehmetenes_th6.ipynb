{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"battal_mehmetenes_th6.ipynb","provenance":[{"file_id":"1gHlqe-gX48HrVOi6l7sqPYHVxT5m_4-b","timestamp":1589327703768}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gzA5v_dYmrKa","colab_type":"text"},"source":["# Take-Home Exam 6: General Review\n","\n","*In this take-home exam, you are going to solve questions regarding machine learning concepts.*\n","\n","**Submission Instructions**\n","\n","---\n","Copy this assignment to your Drive. <font color = 'red'> `File` --> `Save a copy in Drive`</font>. Rename it as <font color = 'green'>`Lastname_Firstname_th6`</font>.\n","\n","Write your solutions in the cells  marked <font color = 'green'>`# your code`</font>.\n","\n","When you're done please submit your solutions as an <font color=\"red\">`.ipynb`</font> file. To do so:\n","\n","\n","1.  Click on <font color=\"red\">`File`</font>  at the top left on the Colab screen, then click on <font color = 'red'>`Download .ipynb`</font>.\n","2.   Then submit the <font color=\"red\">`.ipynb`</font> version of your work on SUCourse.\n","\n","\n","For any question, you may send an email to the TAs and LAs.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"3PPXszHTQ306","colab_type":"text"},"source":["**IMPORTANT**\n","\n","In order to complete the true/false and multi-choice questions, please use markdown utilities. For instance, below you have a set of options \n","\n","- [ ] option A\n","- [x] option B\n","- [ ] option C\n","\n","If you **double click** on this cell, you will see that a ticked option is represented with \\[x\\] statement. Please utilize the same selection mechanism in the questions below."]},{"cell_type":"markdown","metadata":{"id":"HkpA2ho_LNxB","colab_type":"text"},"source":["## ML Concepts"]},{"cell_type":"markdown","metadata":{"id":"KFXTYe2DL7KI","colab_type":"text"},"source":["### Q1\n","\n","What is the difference between classification and regression? Assume that you are performing price prediction. Which is the appropriate method? Explain your reasoning.\n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","Main difference between them is that the output variable in regressin is numerical (or continuous) while it is categorical (or discrete) for classification. \n","Regression models approximate a mapping function f(x) where the x is input variables to a continuous output variable y. y is a real value such as a floating point value and is often used to show quantities. \n","Classification models approximate a mapping function f(x) where x is the input variables to a discrete output variable y. These output variables are often called labels or categories. This function predicts the class (label) of a given input.\n","\n","For price prediction regression is the appropriate method since we are predicting a quantity rather than a category. Quantity can be anything, therefore it is a continuous variable. "]},{"cell_type":"markdown","metadata":{"id":"j4etmk03T6Z1","colab_type":"text"},"source":["### Q2\n","\n","What is overfitting? Explain in terms of its relationship with model complexity and its implications.\n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","Overfitting happens when a model learns the training data too well to the extent of every tiny detail that it negatively impact the performance of the model. Overfitted model perform well on the trainig data but doesn't perform wel on the testing data. An overfit model has low bias and high variance therefore its performance varies widely with unseen data. We can reduce the overfitting by constraining model complexity which means that overfitting can happen when the model is too complex. We can change the complexity by reducing the parameters or changing the structure of a model. For example with decision trees, we can control the complexity by max_depth parameter. "]},{"cell_type":"markdown","metadata":{"id":"f6kMHqNwHRiu","colab_type":"text"},"source":["### Q3\n","\n","Why do we need cross-validation? \n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","We need cross-validation to assess the effectiveness of our machine learning model, particularly when we have to check for overfitting. It is also useful for determining the hyperparameters to achieve the highest accuracy. When we are using classical 3-way partitioning tere are two problems: we can waste a part of our dataset and the accuray can depend on the partitioning of a dataset. Cross validation is the solution for these. It may be computitionally expensive but it is more efficient especially when the dataset is small."]},{"cell_type":"markdown","metadata":{"id":"tkaD4GWbAF0H","colab_type":"text"},"source":["## kNN"]},{"cell_type":"markdown","metadata":{"id":"MQayVyetNhxq","colab_type":"text"},"source":["### Q1\n","\n","A kNN model with k set to 1 would obtain 100% accuracy result on the train data.\n","\n","- [x] True\n","- [ ] False"]},{"cell_type":"markdown","metadata":{"id":"RX8an6jJOlZJ","colab_type":"text"},"source":["### Q2\n","\n","Given a training dataset with 100 observations, we built a kNN model with k set to 100. In the training data, 30% of the observations are of class `A`, 10% are of class `B` and the remaining 60% are of class `C`. \n","\n","Now, we have a test dataset in which 5% are of class `A`, 55% are of class `B` and remaining 40% are of class `C`. If we deploy the trained model to predict the test dataset, what would be the resulting accuracy score?\n","\n","- [ ] 50%\n","- [ ] 40%\n","- [x] 55%"]},{"cell_type":"markdown","metadata":{"id":"YsTpi3s9AHqR","colab_type":"text"},"source":["## Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"jBcChfpoVuhj","colab_type":"text"},"source":["### Q1\n","\n","Suppose we have the training data below, where A, B and C are binary attributes and y is the target attribute.\n","\n","|A|B|C|y|\n","|:---:|:---:|:---:|:---:|\n","|0|1|0|yes|\n","|1|0|1|yes|\n","|0|0|0|no|\n","|1|0|1|no|\n","|0|1|1|no|\n","|1|1|0|yes|\n","\n","Assume we have a decision tree model trained with this model. Is it possible for this model to obtain 100% accuracy score on the training data? Explain your reasoning.\n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","No, it is not possible to obtain 100% accuracy score on the training data simply because of the two contradictory rows. Second and fourth row are the same in features but different at the target. In other words same inputs give different outputs. Even though our model had analized everything correctly, these two rows (A=1, B=0, C=1, Y=Yes and A=1, B=0, C=1, Y=No) would confuse it so that the accuracy would never be 100%."]},{"cell_type":"markdown","metadata":{"id":"_VhPe7Iwczb8","colab_type":"text"},"source":["### Q2\n","\n","Suppose we have the training data below, where A, B and C are binary attributes and y is the target attribute.\n","\n","|A|B|C|y|\n","|:---:|:---:|:---:|:---:|\n","|0|1|0|yes|\n","|1|0|1|yes|\n","|0|0|0|no|\n","|1|0|1|no|\n","|0|1|1|no|\n","|1|1|0|yes|\n","\n","Which attribute above would yield the highest information gain for the root node in the decision tree?\n","\n","- [ ] A\n","- [x] B\n","- [ ] C"]},{"cell_type":"markdown","metadata":{"id":"qrUqwwp1Qn6u","colab_type":"text"},"source":["### Q3\n","\n","A decision tree algorithm iteratively evaluates available features in the data to create the branches. What would be the effect of adding a new feature to the dataset on the predictive performance of a model?\n","\n","- [x] Increases the performance\n","- [ ] Decreases the performance\n","- [ ] There is no enough information"]},{"cell_type":"markdown","metadata":{"id":"2GVphvifLSGq","colab_type":"text"},"source":["## Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"y6oUnpswLToT","colab_type":"text"},"source":["### Q1\n","\n","Why is Naive Bayes algorithm considered as `naive`? Explain briefly and discuss its implications in terms of its effect on the computation.\n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","It is considered naive because it assumes that the features of a measuement are independent from each other. Naive Bayes requires the computation of \n","$P(x_0, \\, â€¦, \\, x_n \\, | \\, c_i)$ which is difficult to compute. In order to simplify the computation all $x_i$; i = 0,1,2,...,n are considered conditionally independent. But this is almost never true. Therefore it is naive."]},{"cell_type":"markdown","metadata":{"id":"PyloVcBXW9o4","colab_type":"text"},"source":["### Q2\n","\n","Suppose that we have the data below, where `Stolen` is the  target attribute that consists of binary values (yes or no).\n","\n","|Color|Type|Origin|Stolen|\n","|:---:|:---:|:---:|:---:|\n","|red|sports|domestic|yes|\n","|red|sports|domestic|no|\n","|red|sports|domestic|yes|\n","|yellow|sports|domestic|no|\n","|yellow|sports|imported|yes|\n","|yellow|suv|imported|no|\n","|yellow|suv|imported|yes|\n","|yellow|suv|domestic|no|\n","|red|suv|imported|no|\n","|red|sports|imported|yes|\n","\n","You have a naive bayes model and a new test instance with attributes `color=red`, `type=suv` and `origin=imported`. What would be the predicted label by the naive bayes model for the given test instance? *Write down your calculations step by step.*\n","\n","There is no need to use latex in this question. You may show your calculations as P(stolen=yes|color=red....) = (...)/(...) etc.\n","\n","<font color=\"blue\">[Double Click to Answer]</font>\n","\n","In this question we need the probabilities of:\n","P(Red|Yes), P(SUV|Yes), P(Imported|Yes), P(Red|No) , P(SUV|No), P(Imported|No) and P(Yes), P(No) to multiply with the respective probabilities.\n","\n","\n","From the data we can calculate these probibilities:\n","\n","P(Red|Yes) = 3/5; P(SUV|Yes) = 1/5; P(Imported|Yes) = 3/5; P(Red|No) = 2/5\n","\n","P(SUV|No) = 3/5; P(Imported|No) = 2/5; p(yes) = 1/2; p(No) = 1/2\n","\n","\n","Since we are trying to find if it is stolen:\n","\n","p(yes|red, suv, imported) = P(Red|Yes) * P(SUV|Yes) * P(Imported|Yes) * p(yes)\n","\n","p(yes|red, suv, imported) = 3/5 * 1/5 * 3/5 * 1/2 = 9/250 = 0.036\n","\n","p(no|red, suv, imported) = P(Red|No) * P(SUV|No) * P(Imported|No) * p(no)\n","\n","p(no|red, suv, imported) = 2/5 * 3/5 * 2/5 * 1/2 = 12/250 = 0.048\n","\n","Since the probability of not getting stolen is bigger than getting stolen (0.048 > 0.036), Naive Bayes would predict the label \"No\" with the given attributes color=red, type=suv and origin=imported."]}]}